
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="中国科学技术大学操作系统课程实验文档">
      
      
        <meta name="author" content="USTC OS Team">
      
      
      
        <link rel="prev" href="ohlab-part2.html">
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14">
    
    
      
        <title>附录 - USTC OS Docs</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.342714a4.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#_1" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../index.html" title="USTC OS Docs" class="md-header__button md-logo" aria-label="USTC OS Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            USTC OS Docs
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              附录
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../index.html" title="USTC OS Docs" class="md-nav__button md-logo" aria-label="USTC OS Docs" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    USTC OS Docs
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../index.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    简介
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    实验一
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            实验一
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../vmlab/vmlab.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Part1 环境准备
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../shelllab/shellab.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Part2 实现Shell
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../syscalllab/syscalllab.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Part3 实现系统调用
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../malloclab/malloclab.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    实验二
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    实验三
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            实验三
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="ohlab-part1.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Part1 开源鸿蒙初探
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="ohlab-part2.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Part2 端侧推理应用实现
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    附录
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="appendix.html" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    附录
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#a-llamacpp" class="md-nav__link">
    <span class="md-ellipsis">
      附录A: 大模型推理与 llama.cpp 简介
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#11-ai" class="md-nav__link">
    <span class="md-ellipsis">
      1.1 人工智能（AI）原理简介
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1.1 人工智能（AI）原理简介">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#111" class="md-nav__link">
    <span class="md-ellipsis">
      1.1.1 什么是人工智能？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#112-machine-learning-ml-ai" class="md-nav__link">
    <span class="md-ellipsis">
      1.1.2 机器学习 (Machine Learning, ML) - AI的核心驱动力
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#113-deep-learning-dl-" class="md-nav__link">
    <span class="md-ellipsis">
      1.1.3 深度学习 (Deep Learning, DL) - 更强大的机器学习
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#12-ai-inference" class="md-nav__link">
    <span class="md-ellipsis">
      1.2 什么是 AI 推理 (Inference)？
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#13-ai-inference-framework" class="md-nav__link">
    <span class="md-ellipsis">
      1.3 什么是 AI 推理框架 (Inference Framework)？
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#14" class="md-nav__link">
    <span class="md-ellipsis">
      1.4 我们实验中接触到的推理工具/框架：
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#15-ai-on-device-ai" class="md-nav__link">
    <span class="md-ellipsis">
      1.5 端侧 AI (On-Device AI)推理 与应用开发简介
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1.5 端侧 AI (On-Device AI)推理 与应用开发简介">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#151-ai" class="md-nav__link">
    <span class="md-ellipsis">
      1.5.1 什么是端侧AI推理？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#152-ai" class="md-nav__link">
    <span class="md-ellipsis">
      1.5.2 端侧AI推理的优势
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#153-ai" class="md-nav__link">
    <span class="md-ellipsis">
      1.5.3 端侧AI推理的挑战：
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#154-ai" class="md-nav__link">
    <span class="md-ellipsis">
      1.5.4 端侧AI应用开发流程（简化版）
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#b" class="md-nav__link">
    <span class="md-ellipsis">
      附录B: 编写自己的库
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#c-llama-democpp" class="md-nav__link">
    <span class="md-ellipsis">
      附录C: llama-Demo.cpp代码说明
    </span>
  </a>
  
    <nav class="md-nav" aria-label="附录C: llama-Demo.cpp代码说明">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      主要函数说明
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#d-native-c-c" class="md-nav__link">
    <span class="md-ellipsis">
      附录D: Native C++ 应用中 C++ 函数定义
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#e-llamacpp" class="md-nav__link">
    <span class="md-ellipsis">
      附录E: llama.cpp的使用
    </span>
  </a>
  
    <nav class="md-nav" aria-label="附录E: llama.cpp的使用">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#e0" class="md-nav__link">
    <span class="md-ellipsis">
      E.0 创建交换分区
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#e1-qwen205b" class="md-nav__link">
    <span class="md-ellipsis">
      E.1 使用qwen2的0.5B模型
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#a-llamacpp" class="md-nav__link">
    <span class="md-ellipsis">
      附录A: 大模型推理与 llama.cpp 简介
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#11-ai" class="md-nav__link">
    <span class="md-ellipsis">
      1.1 人工智能（AI）原理简介
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1.1 人工智能（AI）原理简介">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#111" class="md-nav__link">
    <span class="md-ellipsis">
      1.1.1 什么是人工智能？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#112-machine-learning-ml-ai" class="md-nav__link">
    <span class="md-ellipsis">
      1.1.2 机器学习 (Machine Learning, ML) - AI的核心驱动力
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#113-deep-learning-dl-" class="md-nav__link">
    <span class="md-ellipsis">
      1.1.3 深度学习 (Deep Learning, DL) - 更强大的机器学习
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#12-ai-inference" class="md-nav__link">
    <span class="md-ellipsis">
      1.2 什么是 AI 推理 (Inference)？
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#13-ai-inference-framework" class="md-nav__link">
    <span class="md-ellipsis">
      1.3 什么是 AI 推理框架 (Inference Framework)？
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#14" class="md-nav__link">
    <span class="md-ellipsis">
      1.4 我们实验中接触到的推理工具/框架：
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#15-ai-on-device-ai" class="md-nav__link">
    <span class="md-ellipsis">
      1.5 端侧 AI (On-Device AI)推理 与应用开发简介
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1.5 端侧 AI (On-Device AI)推理 与应用开发简介">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#151-ai" class="md-nav__link">
    <span class="md-ellipsis">
      1.5.1 什么是端侧AI推理？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#152-ai" class="md-nav__link">
    <span class="md-ellipsis">
      1.5.2 端侧AI推理的优势
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#153-ai" class="md-nav__link">
    <span class="md-ellipsis">
      1.5.3 端侧AI推理的挑战：
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#154-ai" class="md-nav__link">
    <span class="md-ellipsis">
      1.5.4 端侧AI应用开发流程（简化版）
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#b" class="md-nav__link">
    <span class="md-ellipsis">
      附录B: 编写自己的库
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#c-llama-democpp" class="md-nav__link">
    <span class="md-ellipsis">
      附录C: llama-Demo.cpp代码说明
    </span>
  </a>
  
    <nav class="md-nav" aria-label="附录C: llama-Demo.cpp代码说明">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      主要函数说明
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#d-native-c-c" class="md-nav__link">
    <span class="md-ellipsis">
      附录D: Native C++ 应用中 C++ 函数定义
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#e-llamacpp" class="md-nav__link">
    <span class="md-ellipsis">
      附录E: llama.cpp的使用
    </span>
  </a>
  
    <nav class="md-nav" aria-label="附录E: llama.cpp的使用">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#e0" class="md-nav__link">
    <span class="md-ellipsis">
      E.0 创建交换分区
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#e1-qwen205b" class="md-nav__link">
    <span class="md-ellipsis">
      E.1 使用qwen2的0.5B模型
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="_1">附录</h1>
<h2 id="a-llamacpp">附录A: 大模型推理与 llama.cpp 简介</h2>
<p>欢迎来到实验的AI探索部分！在本部分，我们将初步接触人工智能（AI）在端侧设备（如我们的DAYU200开发板）上进行推理的概念和实践。由于大家可能之前没有系统学习过AI，我们会从最基本的概念开始。（由于我们是操作系统课程，所以不会很详细(°°)～）</p>
<h2 id="11-ai">1.1 人工智能（AI）原理简介</h2>
<p>你可能经常听到“人工智能”或“AI”这个词，它听起来可能很复杂，但其核心思想其实并不难理解。</p>
<h3 id="111">1.1.1 什么是人工智能？</h3>
<p>简单来说，人工智能的目标是让计算机能够像人一样思考、学习、决策和解决问题。我们希望机器能够执行那些通常需要人类智能才能完成的任务，比如识别图像、理解语言、下棋、驾驶汽车等。</p>
<h3 id="112-machine-learning-ml-ai">1.1.2 机器学习 (Machine Learning, ML) - AI的核心驱动力</h3>
<p>目前实现AI最主要和最成功的方法之一是机器学习 (ML)。与传统编程中我们明确告诉计算机每一步该怎么做不同，机器学习的核心思想是让计算机从数据中“学习”。</p>
<ul>
<li>打个比方： 想象一下教一个小孩识别猫。你不会去精确描述猫的每一根胡须或毛发的数学模型，而是会给她看很多猫的图片（这些图片就是“数据”）。通过观察足够多的例子，小孩的大脑会自己总结出猫的共同特征（比如有尖耳朵、胡须、特定的脸型等——这就是“学习”或“训练模型”的过程）。之后，当你给她看一张新的、她以前没见过的猫的图片时，她也能认出来（这就是“预测”或“推理”）。</li>
<li>机器学习就是让计算机做类似的事情：我们给计算机提供大量的数据（比如成千上万张标记为“猫”或“狗”的图片），并使用特定的算法，让计算机自动从这些数据中找出规律和模式，形成一个“模型 (Model)”。</li>
</ul>
<h3 id="113-deep-learning-dl-">1.1.3 深度学习 (Deep Learning, DL) - 更强大的机器学习</h3>
<p>深度学习是机器学习的一个分支，它使用一种叫做“人工神经网络 (Artificial Neural Networks)”的复杂数学结构，特别是包含许多层（所以叫“深度”）的神经网络。这些网络的设计灵感来源于人脑神经元的连接方式。深度学习在处理复杂数据（如图像、语音、自然语言文本）方面表现得尤其出色，近年来许多AI的突破都得益于深度学习。我们本次实验接触的语言模型就是一种基于深度学习的大语言模型。</p>
<p>总而言之，AI（尤其是通过机器学习和深度学习）使计算机能够通过分析数据来学习和做出智能的响应，而不仅仅是执行预先编程好的指令。</p>
<h2 id="12-ai-inference">1.2 什么是 AI 推理 (Inference)？</h2>
<p>在机器学习（包括深度学习）的生命周期中，通常有两个主要阶段：</p>
<ol>
<li>
<p><strong>训练 (Training)</strong>：</p>
</li>
<li>
<p>这是“学习”的阶段，就像前面例子里小孩看图识猫，或者学生备考学习知识的过程。</p>
</li>
<li>在这个阶段，我们会用海量的标注数据（例如，数百万张图片及其对应的标签，或者TB级的文本数据）来“喂”给一个初始的、未学习的AI模型。</li>
<li>通过特定的学习算法，模型会调整其内部参数，以使其能够对输入数据做出正确的响应（例如，正确分类图片，或理解文本的含义）。</li>
<li>训练过程通常需要非常强大的计算资源（如高性能GPU集群）和很长的时间（几天、几周甚至几个月）。</li>
<li>
<p>训练的最终产出是一个“训练好的模型 (Trained Model)” ——可以把它看作是包含了从数据中学到的所有知识和规律的“大脑”文件。</p>
</li>
<li>
<p><strong>推理 (Inference)</strong>：</p>
</li>
<li>
<p>这是“使用”或“应用”的阶段，就像小孩识别一张新的猫图片，或者学生运用所学知识解答考试题目。</p>
</li>
<li>在这个阶段，我们把新的、未曾见过的数据（例如，用户上传的一张照片，或者用户输入的一段文字）输入到已经训练好的模型中。</li>
<li>模型会利用它在训练阶段学到的知识和规律，对这些新数据进行处理，并给出一个预测、判断或输出（例如，识别出照片中的物体，或者根据输入的文字生成一段新的文字）。</li>
<li>推理过程通常比训练过程快得多，对计算资源的要求也低得多，使其可以在计算能力相对较弱的设备上运行。</li>
</ol>
<p>本实验的核心在于“推理”：我们将使用一个已经训练好的型语言模型 (TinyStory/Qwen)，并在我们的DAYU200开发板上运行它，让它对我们输入的文本进行处理并生成回应。我们不涉及模型的训练过程。</p>
<h2 id="13-ai-inference-framework">1.3 什么是 AI 推理框架 (Inference Framework)？</h2>
<p>当你有了一个训练好的AI模型后，要在实际设备上高效地运行它进行推理，直接操作原始模型文件可能会非常复杂和低效。这时，AI推理框架就派上了用场。</p>
<p>AI推理框架是一套专门设计用来简化和优化在各种硬件平台上部署和执行已训练模型进行推理的软件工具包或库。</p>
<p>它的主要作用包括：</p>
<ul>
<li>模型加载与解析： 能够读取和理解不同格式的训练好的模型文件（例如，TensorFlow产生的.pb文件，PyTorch的.pt文件，MindSpore的.ms文件，或者像Llama.cpp使用的.gguf格式）。</li>
<li>性能优化： 针对目标硬件（如CPU、GPU、NPU——DAYU200上就有NPU）进行深度优化，以加快推理速度、减少内存占用和降低功耗。这可能包括图优化、算子融合、量化、使用特定硬件加速指令等技术。</li>
<li>硬件抽象： 为上层应用提供统一的API接口，屏蔽底层不同硬件的差异。开发者不需要针对每种硬件都写一套不同的代码。</li>
<li>跨平台支持： 很多框架支持将模型部署到多种操作系统（Windows, Linux, Android, iOS, OpenHarmony等）和多种硬件架构上。</li>
<li>易用性： 简化了模型部署的流程，让应用开发者可以更容易地将AI能力集成到自己的应用中。</li>
</ul>
<h2 id="14">1.4 我们实验中接触到的推理工具/框架：</h2>
<ul>
<li>
<p>Llama.cpp: https://github.com/ggml-org/llama.cpp</p>
</li>
<li>
<p>这是一个非常优秀的开源项目，专门用于在CPU上高效运行Llama系列及其他大型语言模型 (LLM)。</p>
</li>
<li>它的主要特点是用纯C/C++编写，追求极致的性能和最小的依赖，非常适合交叉编译到各种端侧设备和嵌入式系统上（比如我们的DAYU200）。</li>
<li>它支持多种量化技术，可以将原本非常庞大的LLM模型压缩到几GB甚至几百MB，同时尽量保持较好的性能，使得在普通PC和资源受限的设备上运行LLM成为可能。</li>
<li>
<p>在本次实验中，我们将把Llama.cpp编译成一个库，并在OpenHarmony应用中调用它来实现语言模型的推理。</p>
</li>
<li>
<p>MindSpore Lite：</p>
</li>
<li>
<p>这是华为昇思MindSpore AI计算框架的轻量化版本，专门用于在端侧设备和物联网设备上进行高效推理。</p>
</li>
<li>我们会提供一个使用MindSpore Lite的Demo。MindSpore Lite支持多种硬件后端（CPU、GPU、以及华为自研的NPU），能够加载MindSpore训练出的模型（.ms格式）以及转换自其他框架（如TensorFlow Lite、ONNX）的模型。</li>
<li>它提供了Java和C++等多种语言的API，方便开发者在Android、iOS、OpenHarmony等系统上集成AI能力。</li>
</ul>
<p>可以把推理框架想象成一个高度专业的“引擎室管理员”，它知道如何最高效地启动和运行一个复杂的“AI引擎”（即训练好的模型），并确保它在特定的“船只”（即你的硬件设备）上表现最佳。</p>
<h2 id="15-ai-on-device-ai">1.5 端侧 AI (On-Device AI)推理 与应用开发简介</h2>
<h3 id="151-ai">1.5.1 什么是端侧AI推理？</h3>
<p>“端侧AI” (On-Device AI 或 Edge AI) 指的是直接在终端用户设备（如智能手机、平板电脑、智能手表、物联网设备、或者我们实验中的DAYU200开发板）上本地执行AI模型的推理过程，而不是将数据发送到远程的云服务器进行处理。</p>
<blockquote>
<p>这也是我们课题组的一个研究方向，欢迎感兴趣的同学联系<a href="http://staff.ustc.edu.cn/~ykli/">李永坤老师</a>，然后加入我们(●'◡'●)</p>
</blockquote>
<h3 id="152-ai">1.5.2 端侧AI推理的优势</h3>
<ul>
<li>低延迟 (Low Latency)： 数据处理在本地进行，无需网络传输，响应速度更快，对于需要实时反馈的应用（如实时语音识别、动态手势识别）至关重要。</li>
<li>隐私保护 (Privacy Protection)： 敏感数据（如个人照片、录音）无需上传到云端，保留在用户本地设备上，更好地保护了用户隐私。</li>
<li>离线可用 (Offline Capability)： 即使在没有网络连接或网络不稳定的情况下，AI功能依然可以正常使用。</li>
<li>节省带宽与成本 (Bandwidth &amp; Cost Saving)： 减少了大量数据上传下载所需的网络带宽，也可能降低了对云端计算资源的依赖，从而节省成本。</li>
</ul>
<blockquote>
<p>其实最主要的是省钱💴还有隐私保护。</p>
</blockquote>
<h3 id="153-ai">1.5.3 端侧AI推理的挑战：</h3>
<ul>
<li>资源限制： 端侧设备的计算能力（CPU、GPU、NPU性能）、内存、存储空间以及电池续航都远不如云服务器。</li>
<li>模型大小与效率： 需要对AI模型进行高度优化和压缩（如量化），使其能够在资源受限的设备上高效运行，同时尽量不损失太多精度。</li>
</ul>
<blockquote>
<p>在本次实验中，我们将体验到量化对模型大小和性能的影响,也会体会到内存和计算能力带来的限制。</p>
</blockquote>
<h3 id="154-ai">1.5.4 端侧AI应用开发流程（简化版）</h3>
<ol>
<li>选择或训练AI模型： 根据应用需求选择合适的预训练模型（例如，是图像分类、目标检测、还是自然语言理解）。在本次实验中，我们直接使用已经训练好的TinyStories模型。</li>
<li>优化与量化： 对模型进行量化（如INT8量化），减小模型大小，提高在端侧设备上的运行效率。</li>
<li>
<p>应用集成与开发：</p>
</li>
<li>
<p>在你的应用程序代码中（对于Llama.cpp，我们将在C++层面操作），调用推理框架提供的API：</p>
</li>
<li>加载优化后的模型文件。</li>
<li>准备输入数据（例如，对图像进行预处理，或将用户文本编码成模型需要的格式）。</li>
<li>执行推理，获取模型输出。</li>
<li>对模型输出进行后处理，并呈现给用户或用于后续逻辑。</li>
</ol>
<blockquote>
<p>在本次实验中，我们将会提供完整的推理代码，同学们只需要编译出推理框架Llama.cpp的动态链接库放到项目代码中即可，如果你对推理的具体逻辑感兴趣，也可以阅读源代码来调整应用的逻辑。</p>
</blockquote>
<div STYLE="page-break-after: always;"></div>

<h2 id="b">附录B: 编写自己的库</h2>
<p>第一部分中，我们学习了如何使用一个第三方动态链接库。同学们可能会好奇，我们如何编写自己的库，并发布给别人呢？本节附录就来简单介绍该过程。为了作为简单起见，我们将演示如何编写只包含一个函数<code>minus_numbers_in_lib</code>的库<code>mylib</code>。</p>
<p>为了实现该库，我们需要两个文件<code>mylib.cpp</code>和<code>mylib.h</code>，前者提供函数的实现，后者定义函数的接口。</p>
<ol>
<li>
<p>创建源代码文件：
   打开终端，创建一个名为<code>mylib.cpp</code>的文件，在该文件中编写一个简单的减法函数如下：</p>
<p><code>cpp
   // 使用 extern "C" 可以防止 C++ 编译器的名称修饰 (name mangling),
   // 使得这些函数更容易被 C 语言或其他语言调用, 或者在不同 C++ 编译器间保持一致性。
   // 对于纯 C++ 内部使用, 且主程序也用同一编译器编译时, 这并非总是必需, 但作为库导出是一种良好实践。
   extern "C" {
   double minus_numbers_in_lib(double a,double b) {
       return a - b;
   }
   }</code></p>
</li>
<li>
<p>为库创建头文件(mylib.h):</p>
</li>
</ol>
<p>在同一文件夹创建<code>mylib.h</code>，声明库中的函数:</p>
<div class="highlight"><pre><span></span><code><span class="cp">#ifndef MYLIB_H</span>
<span class="cp">#define MYLIB_H</span>

<span class="cp">#ifdef __cplusplus</span>
<span class="k">extern</span><span class="w"> </span><span class="s">&quot;C&quot;</span><span class="w"> </span><span class="p">{</span>
<span class="cp">#endif</span>

<span class="c1">// 声明库中导出的函数</span>
<span class="kt">double</span><span class="w"> </span><span class="n">minus_numbers_in_lib</span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="kt">double</span><span class="w"> </span><span class="n">b</span><span class="p">)</span>

<span class="cp">#ifdef __cplusplus</span>
<span class="p">}</span>
<span class="cp">#endif</span>

<span class="cp">#endif </span><span class="c1">// MYLIB_H</span>
</code></pre></div>
<ol>
<li>将源代码交叉编译为动态链接库 (.so 文件):</li>
</ol>
<p>使用以下命令即可：</p>
<div class="highlight"><pre><span></span><code>g++<span class="w"> </span>-shared<span class="w"> </span>-fPIC<span class="w"> </span>mylib.cpp<span class="w"> </span>-o<span class="w"> </span>libmylib.so
</code></pre></div>
<p>为了让其他人更方便使用你的库，你可以将该编译过程写入<code>Makefile</code>。当你的库更复杂，有着更多配置选项时，你可以编写<code>CMakeLists.txt</code>让<code>cmake</code>为你生成<code>Makefile</code>。当然，你还可以让 AI 来帮你编写<code>CMakeLists.txt</code>……（套娃中）。</p>
<p><code>Makefile</code>、<code>CMakeLists.txt</code>等的编写超出了讨论范围，你可以自行查询。</p>
<h2 id="c-llama-democpp">附录C: llama-Demo.cpp代码说明</h2>
<p>本附录旨在简要介绍实验中提供的 llama-Demo.cpp 示例程序。这个程序是一个基于命令行的C++应用，它调用 Llama.cpp 库（即我们编译的 libllama.so）来加载一个大型语言模型（LLM），并根据用户输入的提示（prompt）进行文本推理生成。</p>
<p>理解这个示例程序的工作流程有助于我们了解如何通过API与第三方库进行交互，以实现复杂的功能。</p>
<h3 id="_2">主要函数说明</h3>
<p>以下是对 llama-Demo.cpp 中关键函数功能的简要说明。为简洁起见，部分实现细节将用 ... 表示。</p>
<ul>
<li>头文件导入</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;llama.h&quot;</span><span class="c1"> // 引入Llama.cpp库的公共头文件</span>

<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;cstdio&gt;</span><span class="c1">   // 用于标准输入输出，如printf</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;cstring&gt;</span><span class="c1">  // 用于字符串操作，如strcmp</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;string&gt;</span><span class="c1">   // 用于std::string类</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;vector&gt;</span><span class="c1">   // 用于std::vector类</span>
</code></pre></div>
<ul>
<li>打印程序使用方法与命令行参数解析</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="c1">// 打印程序使用方法</span>
<span class="k">static</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="nf">print_usage</span><span class="p">(</span><span class="kt">int</span><span class="p">,</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">**</span><span class="w"> </span><span class="n">argv</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="p">...</span>
<span class="p">}</span>

<span class="c1">// 解析命令行参数</span>
<span class="kt">void</span><span class="w"> </span><span class="nf">ParseArgs</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">argc</span><span class="p">,</span><span class="w"> </span><span class="kt">char</span><span class="o">**</span><span class="w"> </span><span class="n">argv</span><span class="p">,</span><span class="w"> </span>
<span class="w">            </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&amp;</span><span class="w"> </span><span class="n">model_path</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&amp;</span><span class="w"> </span><span class="n">prompt</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="o">&amp;</span><span class="w"> </span><span class="n">n_predict</span><span class="p">){</span>
<span class="w">                </span><span class="p">...</span>
<span class="p">}</span>
</code></pre></div>
<ul>
<li>加载GGUF模型文件</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="n">llama_model</span><span class="o">*</span><span class="w"> </span><span class="nf">LoadModel</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&amp;</span><span class="w"> </span><span class="n">model_path</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// 设置模型加载参数</span>
<span class="w">    </span><span class="n">llama_model_params</span><span class="w"> </span><span class="n">model_params</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">llama_model_default_params</span><span class="p">();</span>
<span class="w">    </span><span class="c1">// model_params.n_gpu_layers = 0; // 可设置GPU层数，0表示纯CPU。代码中是99，表示尝试全GPU。</span>
<span class="w">    </span><span class="n">model_params</span><span class="p">.</span><span class="n">n_gpu_layers</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">99</span><span class="p">;</span><span class="w"> </span>
<span class="w">    </span><span class="c1">// 加载模型</span>
<span class="w">    </span><span class="n">llama_model</span><span class="o">*</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">llama_model_load_from_file</span><span class="p">(</span><span class="n">model_path</span><span class="p">.</span><span class="n">c_str</span><span class="p">(),</span><span class="w"> </span><span class="n">model_params</span><span class="p">);</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">fprintf</span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;%s: error: unable to load model from &#39;%s&#39;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">__func__</span><span class="p">,</span><span class="w"> </span><span class="n">model_path</span><span class="p">.</span><span class="n">c_str</span><span class="p">());</span>
<span class="w">        </span><span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span><span class="w"> </span><span class="c1">// 加载失败则退出</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;Model loaded successfully: %s</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">model_path</span><span class="p">.</span><span class="n">c_str</span><span class="p">());</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">model</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div>
<ul>
<li>对用户提示进行分词 (Tokenization)。</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">llama_token</span><span class="o">&gt;</span><span class="w"> </span><span class="n">TokenizePrompt</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">llama_vocab</span><span class="o">*</span><span class="w"> </span><span class="n">vocab</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&amp;</span><span class="w"> </span><span class="n">prompt</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// 先估算最大token数量，然后实际进行分词</span>
<span class="w">    </span><span class="c1">// llama_tokenize的参数: vocab, 输入字符串, 长度, 输出buffer, buffer大小, 是否加BOS, 是否特殊处理</span>
<span class="w">    </span><span class="c1">// ... (如代码中所示，先调用一次获取长度，再调用一次填充) ...</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">n_tokens_estimated</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="n">llama_tokenize</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span><span class="w"> </span><span class="n">prompt</span><span class="p">.</span><span class="n">c_str</span><span class="p">(),</span><span class="w"> </span><span class="n">prompt</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span><span class="w"> </span><span class="k">nullptr</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="nb">true</span><span class="p">,</span><span class="w"> </span><span class="nb">true</span><span class="p">);</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">llama_token</span><span class="o">&gt;</span><span class="w"> </span><span class="n">prompt_tokens</span><span class="p">(</span><span class="n">n_tokens_estimated</span><span class="p">);</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">n_tokens_actual</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">llama_tokenize</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span><span class="w"> </span><span class="n">prompt</span><span class="p">.</span><span class="n">c_str</span><span class="p">(),</span><span class="w"> </span><span class="n">prompt</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span><span class="w"> </span><span class="n">prompt_tokens</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span><span class="w"> </span><span class="n">prompt_tokens</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span><span class="w"> </span><span class="nb">true</span><span class="p">,</span><span class="w"> </span><span class="nb">true</span><span class="p">);</span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">n_tokens_actual</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="n">n_tokens_actual</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">n_tokens_estimated</span><span class="w"> </span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="c1">// 出错或数量不对</span>
<span class="w">        </span><span class="n">fprintf</span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;%s: error: failed to tokenize the prompt or size mismatch</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">__func__</span><span class="p">);</span>
<span class="w">        </span><span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">prompt_tokens</span><span class="p">.</span><span class="n">resize</span><span class="p">(</span><span class="n">n_tokens_actual</span><span class="p">);</span><span class="w"> </span><span class="c1">// 调整为实际token数</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">prompt_tokens</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div>
<ul>
<li>初始化上下文与采样器</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="c1">// 初始化推理上下文(context)</span>
<span class="n">llama_context</span><span class="o">*</span><span class="w"> </span><span class="nf">InitializeContext</span><span class="p">(</span><span class="n">llama_model</span><span class="o">*</span><span class="w"> </span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">n_prompt_tokens</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">n_predict</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">llama_context_params</span><span class="w"> </span><span class="n">ctx_params</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">llama_context_default_params</span><span class="p">();</span>
<span class="w">    </span><span class="c1">// 设置上下文窗口大小，必须能容纳提示和生成的最大token数</span>
<span class="w">    </span><span class="n">ctx_params</span><span class="p">.</span><span class="n">n_ctx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">n_prompt_tokens</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">n_predict</span><span class="p">;</span><span class="w"> </span>
<span class="w">    </span><span class="n">ctx_params</span><span class="p">.</span><span class="n">n_batch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">n_prompt_tokens</span><span class="p">;</span><span class="w"> </span><span class="c1">// 初始批处理大小可以设为提示的长度</span>
<span class="w">    </span><span class="c1">// ... </span>
<span class="w">    </span><span class="n">ctx_params</span><span class="p">.</span><span class="n">no_perf</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"> </span><span class="c1">// 开启性能计数</span>

<span class="w">    </span><span class="n">llama_context</span><span class="o">*</span><span class="w"> </span><span class="n">ctx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">llama_init_from_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">ctx_params</span><span class="p">);</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ctx</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">fprintf</span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;%s: error: unable to create Llama context</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">__func__</span><span class="p">);</span>
<span class="w">        </span><span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">ctx</span><span class="p">;</span>
<span class="p">}</span>

<span class="c1">// 初始化采样器(sampler)</span>
<span class="n">llama_sampler</span><span class="o">*</span><span class="w"> </span><span class="nf">InitializeSampler</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">sparams</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">llama_sampler_chain_default_params</span><span class="p">();</span>
<span class="w">    </span><span class="n">sparams</span><span class="p">.</span><span class="n">no_perf</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"> </span><span class="c1">// 开启性能计数</span>
<span class="w">    </span><span class="c1">// ... (可以修改sparams来调整采样策略，例如温度、top_k, top_p等) ...</span>
<span class="w">    </span><span class="n">llama_sampler</span><span class="o">*</span><span class="w"> </span><span class="n">smpl</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">llama_sampler_chain_init</span><span class="p">(</span><span class="n">sparams</span><span class="p">);</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">smpl</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">fprintf</span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;%s: error: Failed to create sampler chain</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">__func__</span><span class="p">);</span>
<span class="w">        </span><span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="c1">// 向采样链中添加一个贪心采样器 (总是选择概率最高的token)</span>
<span class="w">    </span><span class="c1">// 也可以添加其他采样器，如 llama_sampler_init_top_k(), llama_sampler_init_top_p() 等</span>
<span class="w">    </span><span class="n">llama_sampler_chain_add</span><span class="p">(</span><span class="n">smpl</span><span class="p">,</span><span class="w"> </span><span class="n">llama_sampler_init_greedy</span><span class="p">());</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">smpl</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div>
<ul>
<li>文本生成函数</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="kt">void</span><span class="w"> </span><span class="nf">GenerateTokens</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">llama_token</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">prompt_tokens</span><span class="p">,</span><span class="w"> </span><span class="n">llama_context</span><span class="o">*</span><span class="w"> </span><span class="n">ctx</span><span class="p">,</span>
<span class="w">                    </span><span class="k">const</span><span class="w"> </span><span class="n">llama_vocab</span><span class="o">*</span><span class="w"> </span><span class="n">vocab</span><span class="p">,</span><span class="w"> </span><span class="n">llama_sampler</span><span class="o">*</span><span class="w"> </span><span class="n">smpl</span><span class="p">,</span><span class="w"> </span>
<span class="w">                    </span><span class="kt">int</span><span class="w"> </span><span class="n">n_prompt</span><span class="w"> </span><span class="cm">/*初始提示token数*/</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">n_predict</span><span class="w"> </span><span class="cm">/*要生成的最大token数*/</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>

<span class="w">    </span><span class="c1">// 1. 准备包含提示的初始批处理 (batch)</span>
<span class="w">    </span><span class="n">llama_batch</span><span class="w"> </span><span class="n">batch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">llama_batch_get_one</span><span class="p">(</span><span class="n">prompt_tokens</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span><span class="w"> </span><span class="n">n_prompt</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span><span class="w"> </span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">n_current_pos</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="c1">// 当前序列中已处理（解码）的token数量</span>

<span class="w">    </span><span class="c1">// 2. 主生成循环</span>
<span class="w">    </span><span class="c1">// 循环直到达到预测长度，或者遇到序列结束符(EOG)，或者解码失败</span>
<span class="w">    </span><span class="c1">// 代码中循环条件是: n_pos + batch.n_tokens &lt; n_prompt + n_predict</span>
<span class="w">    </span><span class="c1">// 这里的n_pos在代码中是外部循环变量，与batch.n_tokens结合控制总长度。</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">n_pos</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">n_pos</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">batch</span><span class="p">.</span><span class="n">n_tokens</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">n_prompt</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">n_predict</span><span class="p">;</span><span class="w"> </span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// 2a. 解码当前批次 (处理tokens)</span>
<span class="w">        </span><span class="c1">// 第一次循环时，batch中是完整的prompt；后续循环时，batch中是上一步生成的单个token。</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">llama_decode</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span><span class="w"> </span><span class="n">batch</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="c1">// 返回0代表成功</span>
<span class="w">            </span><span class="n">fprintf</span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;%s : failed to eval tokens, return code %d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">__func__</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>
<span class="w">            </span><span class="k">break</span><span class="p">;</span><span class="w"> </span>
<span class="w">        </span><span class="p">}</span>
<span class="w">        </span><span class="n">n_pos</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">batch</span><span class="p">.</span><span class="n">n_tokens</span><span class="p">;</span><span class="w"> </span><span class="c1">// 更新已处理的token总数</span>

<span class="w">        </span><span class="c1">// 2b. 从模型输出中采样下一个token</span>
<span class="w">        </span><span class="n">llama_token</span><span class="w"> </span><span class="n">new_token_id</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">llama_sampler_sample</span><span class="p">(</span><span class="n">smpl</span><span class="p">,</span><span class="w"> </span><span class="n">ctx</span><span class="p">,</span><span class="w"> </span><span class="mi">-1</span><span class="p">);</span><span class="w"> </span><span class="c1">// -1表示从当前上下文最后一个有效token的logits采样</span>

<span class="w">        </span><span class="c1">// 2c. 检查是否是生成结束标记 (End Of Generation)</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">llama_vocab_is_eog</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span><span class="w"> </span><span class="n">new_token_id</span><span class="p">))</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="c1">// printf(&quot;\n[EOG]&quot;); // 遇到结束符，停止生成</span>
<span class="w">            </span><span class="k">break</span><span class="p">;</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="c1">// 2d. 将新生成的token转换为文本并打印</span>
<span class="w">        </span><span class="kt">char</span><span class="w"> </span><span class="n">buf</span><span class="p">[</span><span class="mi">128</span><span class="p">];</span>
<span class="w">        </span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">llama_token_to_piece</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span><span class="w"> </span><span class="n">new_token_id</span><span class="p">,</span><span class="w"> </span><span class="n">buf</span><span class="p">,</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="n">buf</span><span class="p">),</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="nb">true</span><span class="p">);</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">n</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">fprintf</span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;%s: error: failed to convert token to piece</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">__func__</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">s</span><span class="p">(</span><span class="n">buf</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="p">);</span>
<span class="w">        </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;%s&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">s</span><span class="p">.</span><span class="n">c_str</span><span class="p">());</span>
<span class="w">        </span><span class="n">fflush</span><span class="p">(</span><span class="n">stdout</span><span class="p">);</span>
<span class="w">        </span><span class="c1">// 2e. 准备下一个批处理，只包含新生成的token，用于下一次迭代</span>
<span class="w">        </span><span class="n">batch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">llama_batch_get_one</span><span class="p">(</span><span class="o">&amp;</span><span class="n">new_token_id</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>
<h2 id="d-native-c-c">附录D: Native C++ 应用中 C++ 函数定义</h2>
<p>我们在正文中调用了 add 函数，号称其会调用实现在 <code>entry/src/main/cpp</code>里的加法函数。可具体怎么实现的呢？</p>
<p>如正文所说，项目C++相关代码，存放在<code>entry/src/main/cpp</code>目录中，查看该目录，发现目录下有名叫<code>napi_init.cpp</code>的C++文件。打开文件，内容如下（省略部分内容）：</p>
<div class="highlight"><pre><span></span><code><span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;napi/native_api.h&quot;</span>

<span class="k">static</span><span class="w"> </span><span class="n">napi_value</span><span class="w"> </span><span class="nf">Add</span><span class="p">(</span><span class="n">napi_env</span><span class="w"> </span><span class="n">env</span><span class="p">,</span><span class="w"> </span><span class="n">napi_callback_info</span><span class="w"> </span><span class="n">info</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">    </span><span class="c1">// ...</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">sum</span><span class="p">;</span>
<span class="p">}</span>

<span class="n">EXTERN_C_START</span>
<span class="k">static</span><span class="w"> </span><span class="n">napi_value</span><span class="w"> </span><span class="n">Init</span><span class="p">(</span><span class="n">napi_env</span><span class="w"> </span><span class="n">env</span><span class="p">,</span><span class="w"> </span><span class="n">napi_value</span><span class="w"> </span><span class="n">exports</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">    </span><span class="n">napi_property_descriptor</span><span class="w"> </span><span class="n">desc</span><span class="p">[]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="p">{</span><span class="w"> </span><span class="s">&quot;add&quot;</span><span class="p">,</span><span class="w"> </span><span class="k">nullptr</span><span class="p">,</span><span class="w"> </span><span class="n">Add</span><span class="p">,</span><span class="w"> </span><span class="k">nullptr</span><span class="p">,</span><span class="w"> </span><span class="k">nullptr</span><span class="p">,</span><span class="w"> </span><span class="k">nullptr</span><span class="p">,</span><span class="w"> </span><span class="n">napi_default</span><span class="p">,</span><span class="w"> </span><span class="k">nullptr</span><span class="w"> </span><span class="p">}</span>
<span class="w">    </span><span class="p">};</span>
<span class="w">    </span><span class="n">napi_define_properties</span><span class="p">(</span><span class="n">env</span><span class="p">,</span><span class="w"> </span><span class="n">exports</span><span class="p">,</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="n">desc</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="n">desc</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span><span class="w"> </span><span class="n">desc</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">exports</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">EXTERN_C_END</span>

<span class="c1">// ...</span>
</code></pre></div>
<p>文件里有两个奇怪的函数，一个叫<code>Add</code>，看名字是个加法，但内容很复杂，参数和返回值都看不明白。一个叫<code>Init</code>只看得出里面有一行<code>"add"</code>什么的。</p>
<p>实际上，这里的<code>Add</code>函数，就是我们实际调用的 C++ 函数。而<code>Init</code>函数中的下面这行，就是告诉应用，<code>"add"</code>这个<code>ArkTS</code>函数，对应到了<code>Add</code>这个C++函数。</p>
<div class="highlight"><pre><span></span><code><span class="c1">// 这个数组是 napi_property_descriptor 结构体数组</span>
<span class="c1">// 每一行代表一个ArkTS函数和C++函数的映射。其它地方我们不管，第一个&quot;add&quot;代表ArkTS函数的名字（字符串），第三个&quot;Add&quot;代表C++函数（不是字符串，而是函数名，或者相当于函数指针。）</span>
<span class="w">    </span><span class="n">napi_property_descriptor</span><span class="w"> </span><span class="n">desc</span><span class="p">[]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="p">{</span><span class="w"> </span><span class="s">&quot;add&quot;</span><span class="p">,</span><span class="w"> </span><span class="k">nullptr</span><span class="p">,</span><span class="w"> </span><span class="n">Add</span><span class="p">,</span><span class="w"> </span><span class="k">nullptr</span><span class="p">,</span><span class="w"> </span><span class="k">nullptr</span><span class="p">,</span><span class="w"> </span><span class="k">nullptr</span><span class="p">,</span><span class="w"> </span><span class="n">napi_default</span><span class="p">,</span><span class="w"> </span><span class="k">nullptr</span><span class="w"> </span><span class="p">}</span>
<span class="w">    </span><span class="p">};</span>
</code></pre></div>
<p>再仔细看<code>Add</code>函数的内部，调用了很多<code>napi_</code>开头的函数。实际上，这些函数就是负责把<code>add</code>函数的ArkTS格式的参数，转换成C++能够读取的参数。例如：</p>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="kt">double</span><span class="w"> </span><span class="n">value0</span><span class="p">;</span>
<span class="w">    </span><span class="n">napi_get_value_double</span><span class="p">(</span><span class="n">env</span><span class="p">,</span><span class="w"> </span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="o">&amp;</span><span class="n">value0</span><span class="p">);</span>
</code></pre></div>
<p>这两行，将<code>add</code>函数的第一个参数（<code>args[0]</code>），转换为C++中的<code>double</code>，并存放在<code>value0</code>变量中。</p>
<p>而下面两行相反：</p>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="n">napi_value</span><span class="w"> </span><span class="n">sum</span><span class="p">;</span>
<span class="w">    </span><span class="n">napi_create_double</span><span class="p">(</span><span class="n">env</span><span class="p">,</span><span class="w"> </span><span class="n">value0</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">value1</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">sum</span><span class="p">);</span>
</code></pre></div>
<p>创建一个类型为<code>double</code>的ArkTS变量<code>sum</code>，将<code>value0 + value1</code>的值，存入<code>sum</code>中。（后续该值被返回，作为<code>add</code>函数调用的结果。）</p>
<blockquote>
<p>如果你还好奇，这些转换是怎么进行的。你可以发现，这些函数都是<code>napi_</code>开头的，是不是和我们用的<code>llama_</code>系列函数有点像？对的，这个C++文件引用了<code>#include "napi/native_api.h"</code>这个头文件。这实际上是OpenHarmony提供的Native API库的一部分。这个C++文件，实际上就是调用了OpenHarmony SDK中的 Native API 库中的函数，实现了 C++ 变量和ArkTS变量间的转换。</p>
<p>至于具体转换的原理，作为库的使用者，我们是不需要掌握的。这就是调用库的好处。</p>
</blockquote>
<p>实际上，只在C++中定义这个函数还不行，我们还需要在ArkTS中声明这个函数存在，这部分代码在<code>entry/src/main/cpp/types/libentry/Index.d.ts</code>文件中，只有一行：</p>
<div class="highlight"><pre><span></span><code><span class="k">export</span><span class="w"> </span><span class="kd">const</span><span class="w"> </span><span class="nx">add</span><span class="o">:</span><span class="w"> </span><span class="p">(</span><span class="nx">a</span><span class="o">:</span><span class="w"> </span><span class="kt">number</span><span class="p">,</span><span class="w"> </span><span class="nx">b</span><span class="o">:</span><span class="w"> </span><span class="kt">number</span><span class="p">)</span><span class="w"> </span><span class="p">=&gt;</span><span class="w"> </span><span class="kt">number</span><span class="p">;</span>
</code></pre></div>
<p>这里是ArkTS的函数定义的语法，说明我们“导出”了一个<code>add</code>函数，有两个参数<code>a</code>和<code>b</code>，它们都是数字，函数的返回值也是数字。简单说，这行类似下面的C代码：</p>
<div class="highlight"><pre><span></span><code><span class="k">extern</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="nf">add</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">b</span><span class="p">);</span>
</code></pre></div>
<p>是不是理解了呢？就是定义了一个函数罢了。</p>
<p>这样两边都定义之后，通过NAPI库在后台的一系列操作，就能实现ArkTS应用调用C++啦！</p>
<h2 id="e-llamacpp">附录E: llama.cpp的使用</h2>
<p>我们在实验中为了实验的体验，使用了极小的小故事文本生成模型，名字叫做<a href="https://huggingface.co/afrideva/Tinystories-gpt-0.1-3m-GGUF">TinyStory</a>,其由于模型太小，所以产生文本的效果有点差。</p>
<p>那么同学就要问了，有没有表现更好一点的模型呢？答案是：有的兄弟，有的。</p>
<p>我们推荐qwen2的0.5B模型（当然其他的GGUF模型也可以，不过要考虑开发板只有2G运存）。</p>
<h3 id="e0">E.0 创建交换分区</h3>
<p>在运行大型应用程序，尤其是像Llama.cpp这样的大语言模型时，我们可能会遇到开发板物理内存（RAM）不足的情况。为了让系统在这种情况下依然能够运行（尽管性能可能会有所下降），我们可以配置“交换分区”。</p>
<p><strong>什么是交换分区？</strong>
交换分区 (Swap Space)，常被称为“虚拟内存”的一部分，是操作系统在硬盘或其他持久性存储设备（在DAYU200上通常是eMMC内部存储）上划分出来的一块区域。
- 工作原理： 当系统的物理内存 (RAM) 即将耗尽时，操作系统会将RAM中一些暂时不活跃的数据（称为“内存页”）临时移动到交换空间中，从而释放出物理内存给当前更需要的活动进程使用。当那些被移到交换空间的数据再次需要被访问时，操作系统会再将它们从交换空间读回到RAM中。
- 目的：
    - 扩展可用内存： 使得系统能够运行比实际物理内存更大的程序或同时运行更多的程序。
    - 防止内存溢出 (Out-Of-Memory, OOM)： 在物理内存完全用尽时，避免系统因无法分配内存而直接崩溃或杀死重要进程。</p>
<p><strong>如何创建交换分区？</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># hdc shell 进入开发板执行</span>
<span class="nb">cd</span><span class="w"> </span>data/llama_file

<span class="c1"># 在2GB的dayu200上加swap交换空间</span>
<span class="c1"># 新建一个空的ram_ohos文件</span>
touch<span class="w"> </span>ram_ohos
<span class="c1"># 创建一个用于交换空间的文件（8GB大小的交换文件）</span>
dd<span class="w"> </span><span class="k">if</span><span class="o">=</span>/dev/zero<span class="w"> </span><span class="nv">of</span><span class="o">=</span>/data/ram_ohos<span class="w"> </span><span class="nv">bs</span><span class="o">=</span>1G<span class="w"> </span><span class="nv">count</span><span class="o">=</span><span class="m">8</span>
<span class="c1"># 设置文件权限，以确保所有用户可以读写该文件：</span>
chmod<span class="w"> </span><span class="m">777</span><span class="w"> </span>/data/ram_ohos
<span class="c1"># 将文件设置为交换空间：</span>
mkswap<span class="w"> </span>/data/ram_ohos
<span class="c1"># 启用交换空间：</span>
swapon<span class="w"> </span>/data/ram_ohos
<span class="c1"># 查看内存空间:</span>
free<span class="w"> </span>-m
</code></pre></div>
<h3 id="e1-qwen205b">E.1 使用qwen2的0.5B模型</h3>
<blockquote>
<p>如果你只是想要尝试一下，并不想自己构建，直接下载睿客网盘中第二阶段素材中的Demo即可（https://rec.ustc.edu.cn/share/dfbc3380-2b3c-11f0-aee2-27696db61006 中的<code>qwen-Demo.hap</code>）
1. 下载qwen2的0.5B模型，下载地址：https://huggingface.co/Qwen/Qwen2-0.5B-Instruct-GGUF/tree/main ，下载其中qwen2-0_5b-instruct-q4_0.gguf即可
2. 将其放入<code>entry/src/main/resources/resfile</code>文件夹下
3. 修改<code>entry/src/main/ets/pages/Index.ets</code>，修改其中的<code>modelName</code>变量为:
    <div class="highlight"><pre><span></span><code><span class="err">@</span><span class="nx">State</span><span class="w"> </span><span class="nx">modelName</span><span class="o">:</span><span class="w"> </span><span class="nx">string</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;qwen2-0_5b-instruct-q4_0.gguf&#39;</span><span class="p">;</span>
</code></pre></div>
4. 重新构建应用运行即可（由于加载模型时间过长，可能会存在appfreeze导致应用闪退，这是正常现象，真正感兴趣联系助教）效果如下所示：
<img alt="alt text" src="assets/%E6%95%88%E6%9E%9C%E5%9B%BE.jpg" /></p>
</blockquote>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.13a4f30d.min.js"></script>
      
    
  </body>
</html>